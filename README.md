# ðŸ§  MiniGrad: A Minimal Autograd Engine

MiniGrad is a small automatic differentiation engine that was created from the ground up to comprehend the inner workings of backpropagation.  
It is perfect for learning and experimentation because it emphasizes accuracy and clarity over performance.

Motivated by engines in the micrograd style.

---

## Characteristics

Scalar-based automatic differentiation  
Keeps the directed acyclic graph of computations
Reverse-mode autodiff backpropagation  
Overloading operators (`+`, `-`, `*`, `/`, `**`)  
Common nonlinearities are supported.  
Implementation that is clear and readable  

---

## Installation

Make a copy of the repository:

```bash git clone https://github.com/petrovmarko/MarkGrad```
```bash cd MarkGrad```
